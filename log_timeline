11/09/2020:
The day has been spend on litterature search. A number of articles were found which is going to be used in the related work section of the report. 

12/09/2020:
PX4 has been installed. Moreover, a 3D model of the aruco markers has been made. However, the model could not be importet into Gazebo. A solution to this must be found. 12/09/2020:

16/09/2020:
The drone to be used in this project has been implemented as well as the optitrack world in Gazebo. 

04/10/2020:
Arcuo marker detection has been finished where the correct pose from the drone to the marker has been found. Much time has been spend in getting this pose estimation to work proberly. 
This was due to the transformation matrices were not calculated correctly. The ROS method euler_from_matrix(T_drone_marker,'sxyz') was used instead of euler_from_matrix(T_drone_marker,'rxyz'). 
This caused the caulculations to be done wrongly. Also T_drone_camera = euler_matrix(-np.pi/2, np.pi/2,0,'rxyz') np.pi/2 was used instaed of 1.5 which also was a problem. 
Kalman Filters have been implemented for the pose. Maybe better choices of VAR could be found for even better tracking. 

24/10/2020:
Waypoint check fixed. For some reason subcribing to setpoint_local does not work. Strange errors were seen when the local pose of the uav where set to be equal to the wanted setpoint which resultet in that the waypoint check where accomplished strait away. Here changing the waypoint check where the wanted pos where given as input fixed the problem.

26/10/2020:
Kalman filter changed to trust new marker poses more to get a more dynamic update of marker poses in respect to the drone. This solved the problem of vision based navigation in PX4. However, the velocity in the z direction is above max. This could be because of the GPS (Vision pose estimate in the z direction) having a too big variance. This could be solved by using a aruco board for higher precision. This is due to the sensor fusion with IMU, GPS etc. 

2/11/2020:
Marker detection is working with aruco board. However, some of the detected markers in the aruco board seems to be hard to detect. Bigger separation between arucos in the board yeilds higher presicion. This can be due to the arrancement of the aruco on the wall model. The top of the world model should be bigger so that a higher difference between the board and background is made. Cameras uses lot of CPU - Try to lower the cam resolution.

03/10/2020:
ArUco detection for both front and bottom cam works now. However, there seems to be probelms with the translation vector from pose estimation. Here a opposite sign error in the x direction wa observed. This must be fixed! The problem is from the pose from aruco to camera.

04/11/2020:
Pose estimation from aruco to camrera works fine. However, different configurations in regard to the drone control for the front and bottom cam must be set. First step in order to navigate the drone beetween the markers have been made. Here linear interpolation between the waypoints should be used to insure a stabile flight and speed of the drone.

06/11/2020:
First drone navigation between markers performed. However, a fixed speed must be made which is now varying a lot between waypoints. Now the x coordinate is increased by one meter every time a new marker (local coordinate system) is detected. This is done to insure a smooth transition between waypoints which also yiels better stability for the drone. Maybe another solution should be found. 

07/11/2020:
Transformation matrix from camera to drone is wrong somehow!This must be fixed

09/11/2020:
Transformation matrix fixed. Appranthly, the cam orientaion was inverted in respect to the x axis. Normally x is positive horisontally, y positive vertically and z the depth. Here x is inverted yieldig a rotaion matrix of (180,180,0) instead of (0,180,0) in roll, pitch yaw respectivly in regard of the drone to the cam.

10/11/2020:
First iteration of moving between waypoints completed. This first approach takes into account the local coordinate system of each aruco marker. When the drone sees a new aruco board the 'global' coordinate system is increased or decreased by the amount of meters which seperates the markers placed on the ground. This effectively insures a smooth transition betweem the markers and there coordinate system. The drone can be set to move forward, backwards, left or right according to the markers placed on the ground. When moving out to the world the list of markers, movements and waypoints is just reverted. However, the regulation must be tuned because the position control works quite bad at the moment.

Future work:
	* Use the IMU data for position control of the drone (Remember to take drift of this estimation into account). 
	* When the IMU data is used the markers can be used to navigate the drone in a giving direction according to the orientation of the marker. 
	* Moreover, sensor funsion using both IMU and camera data could be used to improve the stabillity of the position control of the drone (Maybe using an Extended Kalman filter).
	From meeting with Henrik:
		* Consider to use only a single aruco board as a sort of global coordinate system. This could be a 100*100 aruco board. Using this no shifting between coordinate systems is done. This 		  could insure a more presice pose estimation as well as better estimation in regard to the PX4 flight control.
		* The test with the error in the pose estimation by the amount of distance from the aruco using interpolation, could also involve how much the aruco board fills out the image of the 			  camera. Here the hypothesis would be, that the more the board occupies in the image frame, the better the pose estimation of the board will be. 
		* Make tests of using bottom cam along, front cam along and both of them for the pose estimation. Give pros and cons from the methods used to summarize which solution could be the best for 			  a given scenario (maybe from a bunsiness's point of view).        
	
Todo:
	* Remember to fix the transformation matrix between the front camera and the drone for the front camera. 
	* The methods in regard to the offset for the x,y and z axis in the aruco pos estimation should be moved away from the marker detection scripts to another place e.g. autonoumous flight. 
	* Hence the detected aruco IDs could be published instead from the marker detection script.
	* Remember to also make a small function to calculate the wanted angle from the giving movements.

05/01/2021:
Final first iteration of the simulation map has been completed. Here a 3d map with a one way opening has been made where the road branches in 3 different directions. It has been tested in all directions and it works quite nice. However, the drone has been found to be quite unstable in keeping its pos in the x and y direction when hovering above a board. This can be due to either a bad chosen PID parameters but more likely the estimated pose estimation from aruco board estimation. A bad camera calibration is not posible because the distortion parameters for the camera have been inizialied ti 0.

06/01/2021:
PID for position has been implemented. However, some issues with the GPS to vision test where it does not change pos has been detected. Tests with the resolution of the cam has been made. Higher resolution yeilds better aruco detection but lower FPS which seems to make the pose control unstable.

07/01/2021:
Test for pose estimation with bottom camera and aruco marker pose follwoing done. Camera matrix for front cam must be made correct and camera matrix for bottom cam must be understood.

08/01/2021:
The transformation matrix from drone to bottom and front camera is now fixed. The reluzation seems to be quite robust in a number of settings. The transformation matrix and hence the rotation from the bottom and front cam must be studied to be perfectly understood. It must however be because I am looking at the settings wrongly. A new model has been made where ONE big aruco board will be used for pose estimation. this will be compared to the other method where each board has its own coordinate system. This concludes the first iteration of where the drone has to follow markers. This iretation has has
been given in detail in the file initial ideas in the data section. 

09/01/2021:
Second iteration where the drone has to follows the markers is now complete. Now instaed of using each marker board as a local coordinate system, a big board is now used which has increased the pose
estimation precision. Moreover, to accommodate for the unstable controller when the local coordinate system of the drone goes from using GPS/local optitrac coordinates to vision based estimation, the 
horizontal and vertical speed as well as the angular velocities of the drone have been set to a minimum of 0.1. This insures that the drone does not become unstable when the transition is made e.g before the new target waypoiont is updated after the transition between local coordinate systems is performed. Futhermore, a small delay of 1 second has been used to insure the new waypoint has been published (maybe this can be neglected). 

Three new final (hopefully) 3D models has been made with a full marker board, threee patterns board and one patterns board. This can be used to see if the pose estimation is decreased if the number of markers in the image is reduced. So far, the drone seems to perform very well even in the one pattern model. 

The third iteration of the follow marker procedure involves better tracking of markers and speed e.g better tuning of the pose controller could be performed and testing of this.   

New todos
	1: Use the IMU data to insure the drone do not fly away if it loses track of the markers. Hence, an estimate of the local position where the IMU data is used must be made. Here, the last known 	    direction of flight could be used to make the drone fly backwards in this direction so the drone can find the markers and contiunes its flightplpan. 
	2: The IMU data can be used to estimate the velocities. If this goes above a limit the drone must switch to using GPS or optitrac to take into account sudden errors in the pose estimation of the 		   markers.

10/01/2021:
New 3D models have been made with aruco going from ID 1 for landing, 100 for wall and 200 for ground markers. Now a landing marker site has been made to initiate a possible landing of the drone with high precision.

11/01/2021:
Different routes frpm gps to landing created. However, there are problems with the transitions from the different marker boards. No problems seems to be by shifting from IMU to camera data when using vision. Maybe it has something to do with a update rate of the imu data being faster then vision. 

14/01/2021:
Now the transitions between different marker boards has been completed. This was done by making a transformation with rotation and transition from the aruco board on the wall (GPS to vision) and landing baords where there coordinate systems have been shiftet with respect to the marker board on the ground (the world marker board). The rotation in the transformation matrix was found with respect to the coordinate system of the drone. Here a rotation around the z axis (-90). Then the position (x,y,z transitions) agian with respect to the coordinate system of the drone. Then an offset of -90 degress around the z axis was set in the marker_detection script because the ground marker is shifted -90 degress around z with respect to the drones starten pose. ALL of this has been done to make the transitions between different coordinate systems as smooth a possible. If this was not done, the control of the drone became unstable. This is possible due to the Kalman filter, where sudden changes of the vision pose results in a lag of the pose estimation and hence leads to an unstable system. Here the new found aruco coordinates was first seen to be giving to the local_vision pose a couple of seconds after the marker was first seen. This has been changed now because a very smooth transition between aruco boards are made because they all are using the same global (ground marker) coordinate system.

21/01/2021:
The tracking script has been changed to sensor_fusion where the IMU data has to be fused along with the pose estimation of the aruco marker. An small rotation matrix has been created to get the IMU data in respect to the ground marker. The accelerometer noise has been found and has to be subtracted from the IMU data as an offset.  

22/01/2021:
The next step is to make an EKF based from the template found on the internet. This will be done from the test section.  
 
