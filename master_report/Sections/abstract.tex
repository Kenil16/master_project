\documentclass[../Head/report.tex]{subfiles}
\begin{document}
\thispagestyle{empty}
\section*{Abstract}

This paper proposes methods for navigation of an unmanned aerial vehicle (UAV) utilizing computer vision. This is achieved using ArUco marker boards which are used for pose estimation of the UAV. The goal is to have a UAV to fly autonomously using GPS in outdoor environments from where missions can be executed. When the UAV needs to recharge, a reliable GPS to vision transition is performed where the UAV uses its front camera to detect an ArUco marker board located on a wall to the entrance of a building. When the UAV is of a certain distance in front of this board located on the wall, it will start using its bottom camera for navigation in indoor GPS denied environments. In the indoor environment, a precise vision based landing can then be performed when it needs to recharge, by using its front camera for pose estimation of ArUco marker boards located in front of the landing stations. For vision based navigation, sensor fusion will be used for pose optimizations.

To achieve autonomous flight for offboard control, the robot operating system (ROS) is used along with the PX4 autopilot. Using ROS, a number of nodes for drone control, autonomous flight, ArUco marker detection and sensor fusion can be run concurrently. The ArUco marker detection and pose estimation is accomplished by using OpenCV and an unscented Kalman filter for sensor fusion. The implementation has been tested in simulation using Gazebo and real flight using a Holybro Pixhawk 4 mini QAV250 Quadcopter UAV with a Raspberry Pi 4B which was used as companion computer where an OptiTrack system has been used for ground truth tracking of the UAV.

Results show that the implementation works well, where the UAV was able to make GPS to vision transitions even in strong wind as well as its ability to use a minimum amount of ArUco markers located on the ground for vision based navigation. The latter was accomplished by successfully implementation of sensor fusion where IMU, barometer and vision data were fused together to achieve reliable pose estimates. The mean error of the precision landings where found to be in the order of centimeters which concludes a reliable implementation of the vision based landings.     

\end{document}