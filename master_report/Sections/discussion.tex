\documentclass[../Head/report.tex]{subfiles}
\begin{document}
\section{Discussion}

From the results in Section \ref{sec:GPS2Vision_pose_estimation} a circular region of poses where the ArUco pose estimation error is quite low was found to be within a maximum distance of approximately 5 meters away from GPS2Vision board. The errors can be seen to be high when the UAV is approximately 5-8 meters away from the GPS2Vision board which goes for both position and angle. From these observations, a \textit{safe area} for the transition between using GPS to vision as navigation would be within a radius of approximately five meters away from the GPS2Vision board which can be seen in Figure \ref{fig:GPS2Vision_pose_estimation_test1_error_pos} and \ref{fig:GPS2Vision_pose_estimation_test1_error_ori}. A possible reason why especially the z position deviates a lot when moving away from the board is that the z component in the coordinate system moves with an angle in roll which triggers the altitude of the UAV to go from positive to negative and hence increases the error in z more than x and y which are less sensitive according to the analysis. It is also quite interesting to see that the position in z is more sensitive when moving away from the board when being straight in front of it. This can be seen in Figure \ref{fig:GPS2Vision_pose_estimation_test1_error_z}. By being further away from the board does not yield the same kind of sensitivity for the position in x and y, but they are more sensitive when facing the board by having an angle as seen in Figure \ref{fig:GPS2Vision_pose_estimation_test1_error_x} and \ref{fig:GPS2Vision_pose_estimation_test1_error_y}.   

Based on the results, the maximum distance away from the GPS2Vision board when executing the GPS to vision transition will be 5 meters with the current configuration of the ArUco marker board and resolution of the camera which was set to $640 \times 480$. However it is possible to reduces the errors by having a better resolution because the detection of markers would be more precise due to the fact the more pixels would yield a better separation of the borders of the markers which in turn produces better pose estimates. This could yield a better performance of the system, but may also stress the Raspberry Pi in regard to the calculations performed. A way to accommodate for that is to use a stronger companion computer e.g Jetson nano, which comes with the expense of a higher price.

As it can be seen in Figure \ref{fig:rolling_average_pos_test1} and \ref{fig:rolling_average_angle_test1} in Section \ref{sec:rolling_average_aruco_pose_estimation}, the fluctuations in the estimates are only of minor concern when the UAV is of a maximum distance of 5 meters from the GPS2Vision board. However, when the UAV changes to a position outside of the \textit{safe area}, the estimations in position fluctuates a lot as seen in Figure \ref{fig:rolling_average_pos_test2}. This could be fatal to make a transition in this scenario and lead to an unstable system. The angle estimates are affected less as seen in Figure \ref{fig:rolling_average_angle_test2}. Hence, the outcome of the rolling average has been used alongside the maximum distance to the GPS2Vision board for stability analysis before initiating the GPS to vision transition. One may argue that being close enough to the ArUco marker board should be an accessible criteria for this transition, but for safety precautions this extra criteria has been considered in the implementation. Nonetheless the calculations of the rolling average puts more pressure on the Raspberry Pi and could be neglected if found redundant.     

It is interesting to see the performance boost in the reduction of the pose estimation error in Section \ref{sec:hold_pose_using_aruco_pose_estimation} when more ArUco markers are visible in the image. Especially by analyzing Table \ref{tab:hold_pose_using_gps2vision_board} and \ref{tab:hold_pose_using_landing_station_boards}, by having the UAV closer to the ArUco marker board decreases the overall pose estimation and target setpoint error significantly. Furthermore, it results in a more robust system because of fluctuations in the pose estimations are less. As it can be seen in test two in Table \ref{tab:hold_pose_using_gps2vision_board}, introducing wind in the simulation only gives negligible changes in the error for pose estimation. However, because the wind effects the placements of the UAV the setpoint error is bigger. This is also seen in Figure \ref{fig:hold_pose_estimation_test2_roll} and \ref{fig:hold_pose_estimation_test2_pitch} where the UAV has a constant offset in roll and pitch due to the applied wind. From this analysis the UAV is capable of keeping its pose even in windy conditions using the implemented control schemes in the PX4 autopilot. The important thing to note here is that the stability of the system depends on a constant flow of pose updates from the ArUco pose estimation and the amount of pose updates at each time instant. Because of the wind, the UAV may come in a situation where it looses sight of the ArUco board which puts a limit to the maximum allowed wind from where the system becomes unstable. From tests this has been found to be in the order of 7-10 $\frac{m}{s}$ from results in \ref{sec:gps2vision_transition}. By analyzing Table \ref{tab:hold_pose_using_landing_station_boards}, it may be noticed the negligible error in the mean position and angle. By examining test 5, the mean error in x, y and z is only 0.9 millimeters and a mean angle of 0.02 degrees. Moreover, the mean error in position in test 3 only adds 3 millimeters to the position and 0.01 degree to the angle from that of test 5. This concludes that the number of markers in the image only adds little gain to the pose estimation. The biggest gain comes by having a sufficient area of the image covered by markers as seen in Figure \ref{fig:hold_pose_aruco_board_three}. The great performance of the ArUco pose estimation can be seen in Figure \ref{fig:hold_pose_estimation_test5_x}, \ref{fig:hold_pose_estimation_test5_y} and \ref{fig:hold_pose_estimation_test5_z} for the positions and \ref{fig:hold_pose_estimation_test5_roll}, \ref{fig:hold_pose_estimation_test5_pitch} and \ref{fig:hold_pose_estimation_test5_yaw} for the angles. 

From this analysis the default settings of the control schemes in the PX4 autopilot for pose control can be used without modifications. However, these parameters may be configured to achieve even better performance for a fine tuning of the system.

The results of testing the implementation of the GPS to vision transition from Section \ref{fig:state_machine_gps2vision_transition} can be seen in Section \ref{sec:gps2vision_transition}. In general it yielded quite good and reliable performance as seen Table \ref{tab:gps2vision_statistics} where almost all runs where completed successfully in a reasonable amount of time. The velocity of the UAV was set to a higher value when using GPS compared to that of using vision (using the GPS2Vision marker board for pose estimates), because instabilities of the system could occur when the UAV was flying towards the GPS2Vision marker board in the last step of the transition as seen in Figure \ref{fig:GPS2Vision_test1_test3}. This instability was due to the UAV having a large angle in roll when flying fast which caused the UAV to loose sight of the GPS2Vision marker board located on the wall. This puts a limit to the current implementation of the system in regard to horizontal velocity. This issue of losing sight of the markers was also observed when the UAV changed the use of camera which can be seen in Figure \ref{fig:gps2vision_vision_navigation}, where the UAV begins to use data from the bottom camera for pose estimation of ArUco markers located on the ground. In this case, the reason for losing sight of the markers located on the ground was due to that a strong wind applied to the simulation in test 3 in Table \ref{tab:gps2vision_statistics}. Here it may be noticed that the UAV where only able to complete 17 out of 20 trials. This is because the UAV lost sight of the markers located on the ground which forced the UAV to land on the ground. As already mentioned, this puts a limit to the implementation where wind speeds of more than 7-10 $\frac{m}{s}$ may cause instabilities to the system and should be avoided. Even though strong winds where applied, the UAV had no problems of navigating to the GPS2Vision marker board as seen in Figure \ref{fig:GPS2Vision_test3}. Here it can be seen that the ground truth of the routes that the UAV takes from an initial position through the substates of the GPS to vision transition, is more disturbed than the one in Figure \ref{fig:GPS2Vision_test1} due to the wind. This concludes that the systems works, but has its limitations because of the fixed cameras attached to the UAV. A way to mitigate this issue, is to use gimbal cameras instead. This type of camera is equipped with motors and sensors which makes it capable of stabilizing its pose. By implementing this solution on the UAV, the just described issues could be solved, and thereby making the system more robust to external influences like wind.     

Results of vision based navigation utilizing sensor fusion can be seen in Section \ref{sec:vision_based_navigation}. The performance of the implementation can be seen in Table \ref{tab:vision_navigation_stastitics} where the used ArUco marker boards can be seen in Figure \ref{fig:vision_navigation_boards}. It is quite remarkable that the error in pose estimation does not seem to grow between test one and two using the configuration in Figure \ref{fig:vision_navigation_full_pattern_board} and \ref{fig:vision_navigation_one_pattern_board} respectively. Here the error is 2.44 centimeters for the mean position and 1.59 and 1.30 degrees for the mean angles. One would expect that the error should increase as the number of ArUco markers decreases in the image. A possible explanation to this is that when the UAV is flying, an hereby having an angle in either roll or pitch, the outermost markers visible in the image gives bad results because of the perspective in the image which causes the resulting error in the ArUco pose estimations to increase. The possibility for this scenario is reduced in the configuration in Figure \ref{fig:vision_navigation_one_pattern_board}, but now a lesser number of markers in the image yields the same corresponding error in the pose estimation. The error first seems to increase when a minimum amount of ArUco markers are located on the ground as seen in the configuration in Figure \ref{fig:vision_navigation_one_pattern_board_missing_markers} and \ref{fig:vision_navigation_one_pattern_board_missing_markers_wear}. This increased error is due to the system does not get the same amount of updates from the ArUco pose estimation and has to rely more on the noisier measurements from IMU and barometers used in the sensor fusion algorithm. However, it must be mentioned that the biggest contributor to the increased error comes from a time delay between the calculations of sensor fusion and the ground truth pose of the UAV. This can be seen in Figure \ref{fig:vision_navigation_error_pos} and \ref{fig:vision_navigation_error_angle}. Here it can be noticed that the sensor fusion pose estimation lacks behind that of the ground truth which is quite easy to see when the UAV changes its pose in a fast manner. This results in big errors especially in the yaw angle when shifting from 180$^{\circ}$ to -180$^{\circ}$ which is seen in Figure \ref{fig:vision_navigation_error_yaw}. Moreover, the estimation of the altitude seen in Figure \ref{fig:vision_navigation_error_z} can be seen to fluctuate quite a lot. This is due to the fusion with barometer data which is more noisy then that of the ArUco pose estimation. Here the measurement noise matrix for barometer measurements could be fine tuned in the UKF to yield better performance by adding more noise to this measurement in the matrix. Even thought a time lack due exist it does not cause the system to be unstable because the UAV was capable of finishing all runs in all tests in Table \ref{tab:vision_navigation_stastitics}. Visualizations of some of these tests can be seen in Figure \ref{fig:vision_navigation_2d_path}. Here a top view of the ground truth path of the UAV can be seen, where it starts in front of the GPS2Vision marker board and then randomly moves to one of the three landing stations and back again to its initial position. From Figure \ref{fig:vision_navigation_2d_path_full_board}, the UAV can be seen to follow the wanted trajectory quite nicely. Here it may be noticed that the UAV do not always end up at the target waypoint which is illustrated by a blue dot. However, this is due to the waypoint check error being 0.1 meters. Lowering this threshold would yield the UAV being closer to the target waypoints, but this would make the UAV wait longer at each waypoint for the system to settle before moving on to the next target. What goes for the test Figure \ref{vision_navigation_2d_path_full_board} and \ref{fig:vision_navigation_2d_path_missing_markers_wear_vel_1.0} is that the horizontal velocity of the UAV was set to 1 $\frac{m}{s}$. However, in Figure \ref{fig:vision_navigation_2d_path_missing_markers_wear_vel_5.0}, a stress test was made to see how well the UAV was able to manage high velocities which is this case was 5 $\frac{m}{s}$. As it can be seen, the UAV does a great job even though a minimum amount of markers is visible. In this case the system relies heavily on measurements from IMU and barometer date in sensor fusion to give updates for new pose estimates to the UAV. Like already mentioned, the reason that the UAV does not follow the trajectory completely, is because the system does not have to settle at each waypoint before moving on to the next due to the choose of waypoint check error. 

It must be mentioned, that no blur was added to the images in the simulations. Blur in the images can make considerable performance decreasement if the camera cannot coup with fast changes in the image. Ways to to take this into account is to alter the settings in aperture or exposure time in the camera configuration. This controls the opening of the lens in regard to aperture and how much light the sensor absorbs before the image is processed in regard to exposure time. Especially the latter can be altered which effectively removes blur in the images, but comes with the expense of the darker image due to lesser amount of light being absorbed in each frame. These considerations much be taken into account for the operational environment in question where a camera with a very high frame rate must be considered if the UAV have to move with great velocities when using the bottom camera to detect markers for pose estimation. 

Another thing to mention is the implementation of the noise estimation in the accelerometer and gyro. As discussed in Section \ref{sec:ukf_implementation}, this noise is estimated by taken ten seconds of measurements when the UAV is steady and then finding the average of these measurements to be subtracted from new measurements of IMU data to effectively remove noise. However, this method could be effected by errors if the UAV does not stand on a completely flat surface. This is no problem in the simulation, but in real life it could lead to errors in the estimation. This could also be due to the PX4 attached to the UAV having a small angle which could lead to the same error. A possible implementation to mitigate this issue is to use the markers to estimate the error in the IMU data as a sort of error state filter. This was actually considered, but not implemented due to time constrains. An implementation proposed by Isaac Skog \cite{GNSSaidedINS} where an EKF propagates and estimates the error states using GPS and IMU data for correction could have been considered implemented in the already created UKF from Section \ref{sec:ukf_implementation}. This way the error could be estimated in flight when reliable pose estimates from ArUco markers were present.    

The results of the landing tests can be seen in Section \ref{sec:vision_based_landing}. A total of six hundred landings where performed in different configurations for testing of the stability of the system. A summarization of the first three hundred landings can be seen in Table \ref{tab:vision_based_landing_statistics_waypoint_error_0.1}. In this configuration, the waypoint check error (WP error) is set to 0.1 meters for both test one, two and three. However the vertical velocities where set to 0.1 $\frac{m}{s}$ in test one, 0.5 $\frac{m}{s}$ in test two and 0.9 $\frac{m}{s}$ in test three. These changes in velocity can clearly be seen to have an effect on the minimum, maximum, mean and STD error. This can be seen by looking at the mean error which is quite acceptable in test one where the mean error is approximately the same for all landing stations with the smallest being a 5.79 cm offset to the target. The increased accuracy comes at the cost of a longer landing time. Here the smallest landing time is 11.51 seconds and smallest stabilization time 0.70 seconds. It may the noticed that the stabilization time in general is much shorter for landing station two for both test one, two and three. A possible reason for this is that the UAV does not have a long flight time before reaching the landing station two from landing station one and three. This will yield a shorter stabilization time before the UAV begins to land. In test 3, the landing time is at the lowest. This yields a general landing time in a few couple of seconds. In this test the mean error is just below the maximum wanted criteria of $\pm 10$ centimeters. However, the worst landing error is in the order of 18.45 centimeters. A visualization of this test can be seen in Figure \ref{fig:vision_based_landing_test3} where the target landing is in the middle of the green circles with an acceptance region of $10$ centimeters in radius. It may be noticed that the final landing positions are primarily located to the left of the target in Figure \ref{fig:vision_based_landing_landing_station_one_test3} and to the right in Figure \ref{fig:vision_based_landing_landing_station_three_test3}. This is because the UAV comes from right to left in Figure \ref{fig:vision_based_landing_landing_station_one_test3} and left to right in Figure \ref{fig:vision_based_landing_landing_station_three_test3} where the UAV is not given time to properly stabilize before the landing is initiated as already mentioned. In this scenario is yields both a bad accuracy as well as precision. In Figure \ref{fig:vision_based_landing_landing_station_two_test3} the accuracy is much better but still with a bad precision. An interesting point to note is that the amount of markers visible in the image does not seem to yield an performance improvement in this configuration with the current setup of waypoint check error and velocities. The overall success rate of the final landings inside of the acceptance region was found to be 92\% for test one, 87\% for test two and 58\% for test three based on the number of runs and success defined in Table \ref{tab:vision_based_landing_statistics_waypoint_error_0.1}. 

In test four, five and six in Table \ref{tab:vision_based_landing_statistics_waypoint_error_0.05}, the waypoint check error can be seen to be decreased which means that the error in waypoint checking must be below 0.05. Using this configuration yields a longer stabilization time, but gives a much better mean error along with a better accuracy and precision in the final landings as seen in Figure \ref{fig:vision_based_landing_test4}. The overall success rate of the final landings inside of the acceptance region was found to be 98\% for test four, 96\% for test five and 93\% for test six based on the number of runs and success defined in Table \ref{tab:vision_based_landing_statistics_waypoint_error_0.05}. 

Compared to similar results from other work the implementation is quite acceptable. In \cite[p.~5]{AutomaticnavigationandlandingofanindoorAR} they achieves an average error of 6 centimeters for a number of ten vision based landings. Here the result is based on real flight tests of a UAV in indoor environments. In \cite[p.~56]{AutonomousRechargingSystemforDronesTwo} they achieves a 10 centimeter offset from the wanted landing position. However, this result was based on outdoor tests on a real implementation of a UAV which was also the case in \cite[p.~6]{AVisionBasedSystemForAutonomousVerticaLanding} who achieves a mean error offset from the wanted landing position of 18 centimeters. Because these results are all based on real implementations of the vision based landing on a UAV, it would not be fair to make comparisons directly. However, if the results from the simulation in Table \ref{tab:vision_based_landing_statistics_waypoint_error_0.05} yields similar performance in real life testing of the UAV, the system would yield better results than the related implementations just discussed. 

The implementation could be improved by altering the PID parameters in the pose control scheme of the PX4 autopilot. This could make the pose controller respond more quickly to changes in the pose of the UAV and hence give a better pose control when new setpoints where given to the system.    

Another improvement to the vision based landing procedure could be to take inspriration from Guanya Shi et al. \cite{NeuralLander} who proposed a deep
learning based robust nonlinear controller. In this implementation a deep neural network (DNN) is used. This setup makes it possible for the system to learn the parameters of the control system which makes it capable of flying very close to the ground. This is because the implementation reduces the aerodynamic ground effect. That way the control system learns to handle the extra airflow caused by the aerodynamic ground effect which yields a smother and precise landing.  

\end{document}