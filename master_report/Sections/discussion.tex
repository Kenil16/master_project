\documentclass[../Head/report.tex]{subfiles}
\begin{document}
\section{Discussion}

From the results in Section \ref{sec:GPS2Vision_pose_estimation} a circular region of poses where the ArUco pose estimation error is quite low was found to be within a maximum distance of approximately 5 meters away from GPS2Vision board. The errors can be seen to be high when the UAV is approximately 5-8 meters away from the GPS2Vision board which goes for both position and angle. From these observations, a \textit{safe area} for the transition between using GPS to vision as navigation would be within a radius of approximately five meters away from the GPS2Vision board which can be seen in Figure \ref{fig:GPS2Vision_pose_estimation_test1_error_pos} and \ref{fig:GPS2Vision_pose_estimation_test1_error_ori}. A possible reason why especially the z position deviates a lot when moving away from the board is that the z component in the coordinate system moves with an angle in roll which triggers the altitude of the UAV to go from positive to negative and hence increases the error in z more than x and y which are less sensitive according to the analysis. It is also quite interesting to see that the position in z is more sensitive when moving away from the board when being straight in front of it. This can be seen in Figure \ref{fig:GPS2Vision_pose_estimation_test1_error_z}. By being further away from the board does not yield the same kind of sensitivity for the position in x and y, but they are more sensitive when facing the board by having an angle as seen in Figure \ref{fig:GPS2Vision_pose_estimation_test1_error_x} and \ref{fig:GPS2Vision_pose_estimation_test1_error_y}.   

Based on the results, the maximum distance away from the GPS2Vision board when executing the GPS to vision transition will be 5 meters with the current configuration of the ArUco marker board and resolution of the camera which was set to $640 \times 480$. However it is possible to reduces the errors by having a better resolution because the detection of markers would be more precise due to the fact the more pixels would yield a better separation of the borders of the markers which in turn produces better pose estimates. This could yield a better performance of the system, but may also stress the Raspberry Pi in regard to the calculations performed. A way to accommodate for that is to use a stronger companion computer e.g Jetson nano, which comes with the expense of a higher price.

As it can be seen in Figure \ref{fig:rolling_average_pos_test1} and \ref{fig:rolling_average_angle_test1} in Section \ref{sec:rolling_average_aruco_pose_estimation}, the fluctuations in the estimates are only of minor concern when the UAV is of a maximum distance of 5 meters from the GPS2Vision board. However, when the UAV changes to a position outside of the \textit{safe area}, the estimations in position fluctuates a lot as seen in Figure \ref{fig:rolling_average_pos_test2}. This could be fatal to make a transition in this scenario and lead to an unstable system. The angle estimates are affected less as seen in Figure \ref{fig:rolling_average_angle_test2}. Hence, the outcome of the rolling average has been used alongside the maximum distance to the GPS2Vision board for stability analysis before initiating the GPS to vision transition. One may argue that being close enough to the ArUco marker board should be an accessible criteria for this transition, but for safety precautions this extra criteria has been considered in the implementation. Nonetheless the calculations of the rolling average puts more pressure on the Raspberry Pi and could be neglected if found redundant.     

It is interesting to see the performance boost in the reduction of the pose estimation error in Section \ref{sec:hold_pose_using_aruco_pose_estimation} when more ArUco markers are visible in the image. Especially by analyzing Table \ref{tab:hold_pose_using_gps2vision_board} and \ref{tab:hold_pose_using_landing_station_boards}, by having the UAV closer to the ArUco marker board decreases the overall pose estimation and target setpoint error significantly. Furthermore, it results in a more robust system because of fluctuations in the pose estimations are less. As it can be seen in test two in Table \ref{tab:hold_pose_using_gps2vision_board}, introducing wind in the simulation only gives negligible changes in the error for pose estimation. However, because the wind effects the placements of the UAV the setpoint error is bigger. This is also seen in Figure \ref{fig:hold_pose_estimation_test2_roll} and \ref{fig:hold_pose_estimation_test2_pitch} where the UAV has a constant offset in roll and pitch due to the applied wind. From this analysis the UAV is capable of keeping its pose even in windy conditions using the implemented control schemes in the PX4 autopilot. The important thing to note here is that the stability of the system depends on a constant flow of pose updates from the ArUco pose estimation and the amount of pose updates at each time instant. Because of the wind, the UAV may come in a situation where it looses sight of the ArUco board which puts a limit to the maximum allowed wind from where the system becomes unstable. From tests this has been found to be in the order of 7-10 $\frac{m}{s}$ from results in \ref{sec:gps2vision_transition}. By analyzing Table \ref{tab:hold_pose_using_landing_station_boards}, it may be noticed the negligible error in the mean position and angle. By examining test 5, the mean error in x, y and z is only 0.9 millimeters and a mean angle of 0.02 degrees. Moreover, the mean error in position in test 3 only adds 3 millimeters to the position and 0.01 degree to the angle from that of test 5. This concludes that the number of markers in the image only adds little gain to the pose estimation. The biggest gain comes by having a sufficient area of the image covered by markers as seen in Figure \ref{fig:hold_pose_aruco_board_three}. The great performance of the ArUco pose estimation can be seen in Figure \ref{fig:hold_pose_estimation_test5_x}, \ref{fig:hold_pose_estimation_test5_y} and \ref{fig:hold_pose_estimation_test5_z} for the positions and \ref{fig:hold_pose_estimation_test5_roll}, \ref{fig:hold_pose_estimation_test5_pitch} and \ref{fig:hold_pose_estimation_test5_yaw} for the angles. 

From this analysis the default settings of the control schemes in the PX4 autopilot for pose control can be used without modifications. However, these parameters may be configured to achieve even better performance for a fine tuning of the system.

The results of testing the implementation of the GPS to vision transition from Section \ref{fig:state_machine_gps2vision_transition} can be seen in Section \ref{sec:gps2vision_transition}. In general it yielded quite good and reliable performance as seen Table \ref{tab:gps2vision_statistics} where almost all runs where completed successfully in a reasonable amount of time. The velocity of the UAV was set to a higher value when using GPS compared to that of using vision (using the GPS2Vision marker board for pose estimates), because instabilities of the system could occur when the UAV was flying towards the GPS2Vision marker board in the last step of the transition as seen in Figure \ref{fig:GPS2Vision_test1_test3}. This instability was due to the UAV having a large angle in roll when flying fast which caused the UAV to loose sight of the GPS2Vision marker board located on the wall. This puts a limit to the current implementation of the system in regard to horizontal velocity. This issue of losing sight of the markers was also observed when the UAV changed the use of camera which can be seen in Figure \ref{fig:gps2vision_vision_navigation}, where the UAV begins to use data from the bottom camera for pose estimation of ArUco markers located on the ground. In this case, the reason for losing sight of the markers located on the ground was due to that a strong wind applied to the simulation in test 3 in Table \ref{tab:gps2vision_statistics}. Here it may be noticed that the UAV where only able to complete 17 out of 20 trials. This is because the UAV lost sight of the markers located on the ground which forced the UAV to land on the ground. As already mentioned, this puts a limit to the implementation where wind speeds of more than 7-10 $\frac{m}{s}$ may cause instabilities to the system and should be avoided. Even though strong winds where applied, the UAV had no problems of navigating to the GPS2Vision marker board as seen in Figure \ref{fig:GPS2Vision_test3}. Here it can be seen that the ground truth of the routes that the UAV takes from an initial position through the substates of the GPS to vision transition, is more disturbed than the one in Figure \ref{fig:GPS2Vision_test1} due to the wind. This concludes that the systems works, but has its limitations because of the fixed cameras attached to the UAV. A way to mitigate this issue, is to use gimbal cameras instead. This type of camera is equipped with motors and sensors which makes it capable of stabilizing its pose. By implementing this solution on the UAV, the just described issues could be solved, and thereby making the system more robust to external influences like wind.     

Results of vision based navigation utilizing sensor fusion can be seen in Section \ref{sec:vision_based_navigation}. The performance of the implementation can be seen in Table \ref{tab:vision_navigation_stastitics} where the used ArUco marker boards can be seen in Figure \ref{fig:vision_navigation_boards}. It is quite remarkable that the error in pose estimation does not seem to grow between from test one and two using the configuration in Figure \ref{fig:vision_navigation_full_pattern_board} and \ref{fig:vision_navigation_one_pattern_board} respectively. Here the error is 2.44 centimeters for the mean position and 1.59 and 1.30 degrees for the mean angles. One would expect that the error should increase as the number of ArUco markers decreases in the image. A possible explanation to this is that when the UAV is flying, an hereby having an angle in either roll or pitch, the outermost markers visible in the image gives bad results because of the perspective in the image which causes the resulting error in the ArUco pose estimations to increase. The possibility for this scenario is reduced in the configuration in Figure \ref{fig:vision_navigation_one_pattern_board}, but now a lesser number of markers in the image yields the same corresponding error in the pose estimation. The error first seems to increase when a minimum amount of ArUco markers are located on the ground as seen in the configuration in Figure \ref{fig:vision_navigation_one_pattern_board_missing_markers} and \ref{fig:vision_navigation_one_pattern_board_missing_markers_wear}. This increased error is due to the system does not get the same amount of updates from the ArUco pose estimation and has to rely more on the noisier measurements from IMU and barometers used in the sensor fusion algorithm. However, it must be mentioned that the biggest contributor to the increased error comes from a time delay between the calculations of sensor fusion and the ground truth pose of the UAV. This can be seen in Figure \ref{fig:vision_navigation_error_pos} and \ref{fig:vision_navigation_error_angle}. Here it can be noticed that the sensor fusion pose estimation lacks behind that of the ground truth which is quite easy to see when the UAV changes its pose in a fast manner. This results in big errors especially in the yaw angle when shifting from 180$^{\circ}$ to -180$^{\circ}$ which is seen in Figure \ref{fig:vision_navigation_error_yaw}. Moreover, the estimation of the altitude seen in Figure \ref{fig:vision_navigation_error_z} can be seen to fluctuate quite a lot. This is due to the fusion with barometer data which is more noisy then that of the ArUco pose estimation. Here the measurement noise matrix for barometer measurements could be fine tuned UKF to yield better performance by adding more noise to this measurement in the matrix. 


 
As mentioned in Table \ref{tab:vision_navigation_stastitics}, the estimated error is quite high due to a time delay between the ground truth pose of the drone and the estimations from sensor fusion. This can be seen in Figures \ref{fig:vision_navigation_error_pos} and \ref{fig:vision_navigation_error_angle} where the estimations of the pose follows the ground truth quite nicely, but the errors arises when large changes in the pose occurs. This scenario is likely to occur in real life because the Raspberry Pi, which would be responsible for calculations of the pose, would have a sparse amount of resources available compared to that of the computer in the simulated environment. Even though the estimated pose lack behind compared to the ground truth, the stability of the system does not decrease.   

As seen in Figure \ref{fig:vision_navigation_2d_path}, the drone follows the target trajectories nicely. In Figure \ref{fig:vision_navigation_2d_path_missing_markers_wear_vel_5.0}, the drone seems to deviate quite a bit from the target trajectory, but that is mostly due to horizontal velocity being $5 \frac{m}{s}$ which tends to be in upper threshold for the maximum velocity in the current configuration if the trajectories are important to keep. 

Even though the drone deviates from the wanted paths, the target waypoints can be seen to be followed nicely. This gives an indication of the robustness of the current implementation using sensor fusion, where the drone does not decreases in stability even though the markers are only minimally visible and only in some of the images. 

The results of a total of six hundred landings can be seen in Tables \ref{tab:vision_based_landing_statistics_waypoint_error_0.1} and \ref{tab:vision_based_landing_statistics_waypoint_error_0.05}. It can be seen that the vertical velocity along with the waypoint error plays a major role in the error in the final landing position in the implemented configuration. This is because the drone needs to settle when placed in front of the ArUco landing board before initiating its landing procedure. Moreover, there does not seem to be a performance boost by using an ArUco board with a lot of markers, because the biggest challenge in this configuration is the actual pose control in the PX4 software which would have to be fine-tuned to achieve even better performance. This could be done by changing the parameters in the PID controllers for position and velocity control of the drone. However, because this fine-tuning of the PID parameters would be case specific, could change a lot when moving to the actual drone, the default configuration of the PID parameters will be kept as default with possible changes when moving on with the actual drone in real life. 

Visualizations of the final landing positions seen from above can be seen in Figures \ref{fig:vision_based_landing_test3} and \ref{fig:vision_based_landing_test4}. It may be noticed that the error is bigger for landing station one and three in \ref{fig:vision_based_landing_landing_station_one_test3} and \ref{fig:vision_based_landing_landing_station_three_test3} respectively. This is because the drone could have moved between landing station one and three or landing station three to one where the system has not settled properly before performing the landing procedure. This is exactly the problem which is solved when decreasing the waypoint error in Figure \ref{fig:vision_based_landing_test4} which can clearly be seen to have a positive impact on the final landing error in Figures \ref{fig:vision_based_landing_landing_station_one_test4} and \ref{fig:vision_based_landing_landing_station_three_test4}.  

In general, a maximum final landing error of $\pm$ 10 centimeters is achieved when the checkpoint and vertical velocities are chosen correctly as in test 4, with only minor deviations from the maximum allowed landing position error. 

\end{document}