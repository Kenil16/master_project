\documentclass[../Head/report.tex]{subfiles}
\begin{document}
\section{Discussion}

The errors can be seen to be high when the drone is approximately 5-8 meters away from the GPS2Vision board which goes for both position and angle. From these observations, a \textit{safe area} for the transition between using GPS to vision as navigation would be within a radius of approximately five meters away from the GPS2Vision board which can be seen in Figure \ref{fig:GPS2Vision_pose_estimation_test1_error_pos} and \ref{fig:GPS2Vision_pose_estimation_test1_error_ori}. A possible reason why especially the z position deviates a lot when moving away from the board is that the z component in the coordinate system moves with an angle in roll which triggers the altitude of the drones to go from positive to negative and hence increases the error in z more than x and y which are less sensitive according to the analysis. 

Based on the results, the maximum distance away from the GPS2Vision board when executing the GPS to vision transition will be 5 meters which will be used in the implementation of the code.

As it can be seen in Figures \ref{fig:rolling_average_pos_test1} and \ref{fig:rolling_average_angle_test1}, the fluctuations in the estimates are only of minor concern when the drone is of a maximum distance of 5 meters from the GPS2Vision board. However, when the drone changes to a position outside of the \textit{safe area}, the estimations in position fluctuates a lot as seen in Figure \ref{fig:rolling_average_pos_test2}. This could be fatal to make a transition in this scenario and lead to an unstable system. The angle estimates are affected less as seen in Figure \ref{fig:rolling_average_angle_test2}.

The outcome of the rolling average will be used alongside the maximum distance to the GPS2Vision board for stability analysis before going from GPS to vision navigation in Section \ref{sec:gps2vision_transition}.


As analyzed in Tables \ref{tab:hold_pose_using_gps2vision_board} and \ref{tab:hold_pose_using_landing_station_boards}, having the drone closer to the ArUco marker board decreases the overall pose estimation and target setpoint error. Furthermore, it results in a more robust pose control system, because of fluctuations in the pose estimations is less. 

From this analysis it has been decided to move on with the current configuration of the system, keeping the variables in the PID controllers in the PX4 firmware as they are by default. However, these parameters may be configured when moving on with the implementation on the actual drone. But based on the simulations, the system operates quite stable as long as the distances from the ArUco marker boards are kept as they are in the simulations.   

As mentioned in Table \ref{tab:gps2vision_statistics}, the drone got a hard time finding the ArUco markers located on the ground if the simulated wind was too high which was found to be the case in test 3. This could cause issues when incorporating the implementation on the actual drone in windy conditions. A possible solution to this problem would be to install a gimbal camera on the drone, which could insure a stable pose of the camera no matter the actual angle in roll and pitch of the drone. This could effective solve this issue and improve the overall stability of the system, because the perspective in the image, caused from an increased angle in roll and pitch of the drone, would be minimized.

The test also shows that the drone has no problems of finding and navigating to the GPS2Vision board, and issues first arise in the transition of GPS to vision due to the angle of the drone as already mentioned. In Figure \ref{fig:GPS2Vision_test3}, the path of the drone can be seen to fluctuate a bit due to the simulated wind. However, this do not decreases the stability of the system only a longer flying time as seen in Table \ref{tab:gps2vision_statistics}. 

As mentioned in Table \ref{tab:vision_navigation_stastitics}, the estimated error is quite high due to a time delay between the ground truth pose of the drone and the estimations from sensor fusion. This can be seen in Figures \ref{fig:vision_navigation_error_pos} and \ref{fig:vision_navigation_error_angle} where the estimations of the pose follows the ground truth quite nicely, but the errors arises when large changes in the pose occurs. This scenario is likely to occur in real life because the Raspberry Pi, which would be responsible for calculations of the pose, would have a sparse amount of resources available compared to that of the computer in the simulated environment. Even though the estimated pose lack behind compared to the ground truth, the stability of the system does not decrease.   

As seen in Figure \ref{fig:vision_navigation_2d_path}, the drone follows the target trajectories nicely. In Figure \ref{fig:vision_navigation_2d_path_missing_markers_wear_vel_5.0}, the drone seems to deviate quite a bit from the target trajectory, but that is mostly due to horizontal velocity being $5 \frac{m}{s}$ which tends to be in upper threshold for the maximum velocity in the current configuration if the trajectories are important to keep. 

Even though the drone deviates from the wanted paths, the target waypoints can be seen to be followed nicely. This gives an indication of the robustness of the current implementation using sensor fusion, where the drone does not decreases in stability even though the markers are only minimally visible and only in some of the images. 

The results of a total of six hundred landings can be seen in Tables \ref{tab:vision_based_landing_statistics_waypoint_error_0.1} and \ref{tab:vision_based_landing_statistics_waypoint_error_0.05}. It can be seen that the vertical velocity along with the waypoint error plays a major role in the error in the final landing position in the implemented configuration. This is because the drone needs to settle when placed in front of the ArUco landing board before initiating its landing procedure. Moreover, there does not seem to be a performance boost by using an ArUco board with a lot of markers, because the biggest challenge in this configuration is the actual pose control in the PX4 software which would have to be fine-tuned to achieve even better performance. This could be done by changing the parameters in the PID controllers for position and velocity control of the drone. However, because this fine-tuning of the PID parameters would be case specific, could change a lot when moving to the actual drone, the default configuration of the PID parameters will be kept as default with possible changes when moving on with the actual drone in real life. 

Visualizations of the final landing positions seen from above can be seen in Figures \ref{fig:vision_based_landing_test3} and \ref{fig:vision_based_landing_test4}. It may be noticed that the error is bigger for landing station one and three in \ref{fig:vision_based_landing_landing_station_one_test3} and \ref{fig:vision_based_landing_landing_station_three_test3} respectively. This is because the drone could have moved between landing station one and three or landing station three to one where the system has not settled properly before performing the landing procedure. This is exactly the problem which is solved when decreasing the waypoint error in Figure \ref{fig:vision_based_landing_test4} which can clearly be seen to have a positive impact on the final landing error in Figures \ref{fig:vision_based_landing_landing_station_one_test4} and \ref{fig:vision_based_landing_landing_station_three_test4}.  

In general, a maximum final landing error of $\pm$ 10 centimeters is achieved when the checkpoint and vertical velocities are chosen correctly as in test 4, with only minor deviations from the maximum allowed landing position error. 

\end{document}