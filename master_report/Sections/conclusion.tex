\documentclass[../Head/report.tex]{subfiles}
\begin{document}
\section{Conclusion}
\label{sec:conclusion}

The implementation of the offboard control system was based on ROS and the PX4 autopilot. This way a number of nodes in the robot operating system for drone control, autonomous flight, pose estimation using ArUco marker boards and sensor fusion could be run concurrently in order to achieve autonomous flight of the UAV for Requirement \ref{sor:one}. Furthermore, this software was tested in Gazebo simulations where a number of missions could be set to be executed autonomously using keyboard commands. This software was fetched to a Raspberry Pi 4B which was used as companion computer for the Holybro Pixhawk 4 mini QAV250 Quadcopter UAV. Hence, using the wireless module of the Raspberry Pi 4B for wireless communication to the laptop, these missions could be send through a wireless connection to be executed on the UAV. This satisfies Requirement \ref{sor:three}.     

In order to use computer vision for pose estimation, a Python implementation of OpenCV was used. This gives the opportunity of detecting ArUco marker boards, where the pose estimation error were in the range of millimeters if the UAV was placed approximately one meter in front of the ArUco marker board. Moreover, it can be concluded that if more markers are visible in the image, better pose estimations can be achieved. For navigation between ArUco marker boards, transformations of the estimated ArUco marker boards for landing and the board used in the GPS to vision transition were performed to align these coordinate systems to the one located on the ground. This ensured that only a single coordinate system would have to be used for vision based pose estimation. This satisfies Problem \ref{ps:one}, \ref{ps:three} and \ref{ps:five}.

For satisfying Problem \ref{ps:two}, initial testing of the ArUco pose estimation was performed to see how far away the UAV could be from the GPSVision board before initiating the GPS to vision transition without causing instabilities to the vision based pose estimation. Results showed that the UAV had to be in a radius of maximum five meters away from the GPS2Vision ArUco marker board where fluctuations in the estimated pose were accessible giving that the GPS2Vision board being a ($2 \times 8 $) ArUco marker board with marker size of 0.2 meters and distance between markers of 0.1 meters. The resolution of the camera was $640 \times 480$. This way the UAV had to navigate to the GPS2Vision ArUco marker board using GPS until being close enough to the board to make a reliable GPS to vision transition. Moreover, the coordinate system of the ArUco marker board located on the ground was transformed to be aligned to the local coordinate system of the UAV when using GPS. This resulted in a smooth GPS to vision transition, because no large change in the pose were seen due to the fact that the original coordinate system was used. Hence, increments or decrements to the original pose would just be added yielding a stable   flight control using the PX4 autopilot.

Furthermore, this implementation was tested in a number of different configurations with applied wind for stress analysis. Results showed that the implementation of the GPS2Vision transition were robust to applied wind in the simulations and managed to complete all runs with wind speeds up to 5-7 $\frac{m}{s}$. However, only seventeen out of twenty runs were completed with wind speeds up to 7-10 $\frac{m}{s}$ which puts an upper threshold to the current implementation of the system for maximum allowed wind before the system becomes unstable. This satisfies Requirement \ref{sor:four}. However, due to time constraints, the GPS to vision transition was only performed in simulations. 

Sensor fusion was implemented to take into account scenarios where no ArUco markers where visible for the bottom camera of the UAV in vision based navigation. Here an unscented kalman filter were implemented. Results showed that the implementation worked as expected where pose estimations from ArUco markers, IMU and barometer data where fused together yielding sensor fusion pose estimation. The implementation was tested in a number of different configuration where small errors in the range of centimeters and degrees where observed. This satisfies Problem \ref{ps:four}. However, due to time constraints, the implementation of sensor fusion was only tested in simulations.

The vision based landings where accomplished by having three different sets of ArUco marker boards. This was done to see if the number of visible amount of markers in the image yielded better final landings. Results showed a lowest mean error of 4.54 cm with overall landing time of 20.69 seconds in the simulation for 34 runs and 6.99 mean error with overall landing time of 7.75 seconds for 20 runs in real flight of the UAV. Thought Requirement \ref{sor:two} is satisfied, improvements to the system must be made to satisfy Requirement \ref{sor:five}. The velocity of the UAV can be increased to reduce the landing time, but it comes with an increased error in the landing position. This completes Problem \ref{ps:six}. Further analysis showed no significant improvements of using the three different landing boards in regard to the mean position landing error.

\end{document}